{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfee4140-d278-4b73-aa19-5a679a67fccd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"functional_test\", \"farmers\", [\"farmers\", \"retail\", \"okta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34760538-070c-4a02-9bf7-e1ebef66dec7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def create_job(job_name):\n",
    "    url = f\"https://adb-362282074994262.2.azuredatabricks.net/api/2.0/jobs/create\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {dbutils.secrets.get('myscope', 'databricks-token')}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"name\": job_name,\n",
    "        \"email_notifications\": {\n",
    "            \"on_success\": [\"charles@email.com\"],\n",
    "            \"on_failure\": [\"charles@email.com\"]\n",
    "        },\n",
    "        \"new_cluster\": {\n",
    "            \"spark_version\": \"15.4.x-scala2.12\",\n",
    "            \"node_type_id\": \"Standard_DS3_v2\",\n",
    "            \"num_workers\": 2\n",
    "        },\n",
    "        \"tasks\": [\n",
    "            {\n",
    "                \"task_key\": \"initial_task\",\n",
    "                \"notebook_task\": {\n",
    "                    \"notebook_path\": \"/Path/To/Your/Notebook\"\n",
    "                },\n",
    "                \"new_cluster\": {\n",
    "                    \"spark_version\": \"15.4.x-scala2.12\",\n",
    "                    \"node_type_id\": \"Standard_DS3_v2\",\n",
    "                    \"num_workers\": 2\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"task_key\": \"dependent_task\",\n",
    "                \"depends_on\": [\n",
    "                    {\n",
    "                        \"task_key\": \"initial_task\"\n",
    "                    }\n",
    "                ],\n",
    "                \"notebook_task\": {\n",
    "                    \"notebook_path\": \"/Path/To/Your/DependentNotebook\"\n",
    "                },\n",
    "                \"new_cluster\": {\n",
    "                    \"spark_version\": \"15.4.x-scala2.12\",\n",
    "                    \"node_type_id\": \"Standard_DS3_v2\",\n",
    "                    \"num_workers\": 2\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    if response.status_code == 200:\n",
    "        job_id = response.json().get(\"job_id\")\n",
    "        print(f\"Job {job_name} created successfully with job_id: {job_id}\")\n",
    "        return job_id\n",
    "    else:\n",
    "        print(f\"Failed to create job {job_name}: {response.text}\")\n",
    "        return None\n",
    "\n",
    "def start_job_run(job_id, parameters):\n",
    "    url = f\"https://adb-362282074994262.2.azuredatabricks.net/api/2.0/jobs/run-now\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {dbutils.secrets.get('myscope', 'databricks-token')}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"job_id\": job_id,\n",
    "        \"notebook_params\": parameters\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Job run for job_id {job_id} started successfully\")\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Failed to start job run for job_id {job_id}: {response.text}\")\n",
    "        return None\n",
    "\n",
    "functional_tests = [\"farmers\", \"retail\", \"okta\"]\n",
    "job_ids = {}\n",
    "\n",
    "for test in functional_tests:\n",
    "    job_id = create_job(test)\n",
    "    if job_id:\n",
    "        job_ids[test] = job_id\n",
    "\n",
    "for test, job_id in job_ids.items():\n",
    "    parameters = {\"functional_test\": test}\n",
    "    start_job_run(job_id, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a9d93ee-1e6d-4149-abdc-fe1d0ba95630",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### DLT Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f8c0eab-bfa9-49e1-9a52-8d1711e7ea62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def create_dlt_pipeline(pipeline_name, notebook_path):\n",
    "    url = f\"https://adb-362282074994262.2.azuredatabricks.net/api/2.0/pipelines\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {dbutils.secrets.get('myscope', 'databricks-token')}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"name\": pipeline_name,\n",
    "        \"storage\": \"/path/to/storage\",\n",
    "        \"configuration\": {\n",
    "            \"notebook_path\": notebook_path\n",
    "        },\n",
    "        \"clusters\": [\n",
    "            {\n",
    "                \"label\": \"default\",\n",
    "                \"num_workers\": 2\n",
    "            }\n",
    "        ],\n",
    "        \"libraries\": [],\n",
    "        \"target\": \"default\",\n",
    "        \"continuous\": False,\n",
    "        \"development\": True\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    if response.status_code == 200:\n",
    "        pipeline_id = response.json().get(\"pipeline_id\")\n",
    "        print(f\"DLT pipeline {pipeline_name} created successfully with pipeline_id: {pipeline_id}\")\n",
    "        return pipeline_id\n",
    "    else:\n",
    "        print(f\"Failed to create DLT pipeline {pipeline_name}: {response.text}\")\n",
    "        return None\n",
    "\n",
    "def create_job(job_name, pipeline_id):\n",
    "    url = f\"https://adb-362282074994262.2.azuredatabricks.net/api/2.0/jobs/create\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {dbutils.secrets.get('myscope', 'databricks-token')}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"name\": job_name,\n",
    "        \"email_notifications\": {\n",
    "            \"on_success\": [\"charles@email.com\"],\n",
    "            \"on_failure\": [\"charles@email.com\"]\n",
    "        },\n",
    "        \"tasks\": [\n",
    "            {\n",
    "                \"task_key\": \"initial_task\",\n",
    "                \"notebook_task\": {\n",
    "                    \"notebook_path\": \"/Path/To/Your/Notebook\"\n",
    "                },\n",
    "                \"new_cluster\": {\n",
    "                    \"spark_version\": \"15.4.x-scala2.12\",\n",
    "                    \"node_type_id\": \"Standard_DS3_v2\",\n",
    "                    \"num_workers\": 2\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"task_key\": \"dlt_task\",\n",
    "                \"depends_on\": [\n",
    "                    {\n",
    "                        \"task_key\": \"initial_task\"\n",
    "                    }\n",
    "                ],\n",
    "                \"pipeline_task\": {\n",
    "                    \"pipeline_id\": pipeline_id\n",
    "                },\n",
    "                \"new_cluster\": {\n",
    "                    \"spark_version\": \"15.4.x-scala2.12\",\n",
    "                    \"node_type_id\": \"Standard_DS3_v2\",\n",
    "                    \"num_workers\": 2\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    if response.status_code == 200:\n",
    "        job_id = response.json().get(\"job_id\")\n",
    "        print(f\"Job {job_name} created successfully with job_id: {job_id}\")\n",
    "        return job_id\n",
    "    else:\n",
    "        print(f\"Failed to create job {job_name}: {response.text}\")\n",
    "        return None\n",
    "\n",
    "def start_job_run(job_id, parameters):\n",
    "    url = f\"https://adb-362282074994262.2.azuredatabricks.net/api/2.0/jobs/run-now\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {dbutils.secrets.get('myscope', 'databricks-token')}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"job_id\": job_id,\n",
    "        \"notebook_params\": parameters\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Job run for job_id {job_id} started successfully\")\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Failed to start job run for job_id {job_id}: {response.text}\")\n",
    "        return None\n",
    "\n",
    "functional_tests = [\"farmers\", \"retail\", \"okta\"]\n",
    "job_ids = {}\n",
    "\n",
    "for test in functional_tests:\n",
    "    pipeline_id = create_dlt_pipeline(f\"{test}_pipeline\", f\"/Path/To/{test}/Notebook\")\n",
    "    if pipeline_id:\n",
    "        job_id = create_job(test, pipeline_id)\n",
    "        if job_id:\n",
    "            job_ids[test] = job_id\n",
    "\n",
    "for test, job_id in job_ids.items():\n",
    "    parameters = {\"functional_test\": test}\n",
    "    start_job_run(job_id, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82794568-45be-4ab4-ac11-fabf996af79f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Start DLT Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcb27b5e-5df8-4dd7-9cb9-9070907f2eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def start_dlt_pipeline(pipeline_id):\n",
    "    url = f\"https://adb-362282074994262.2.azuredatabricks.net/api/2.0/pipelines/{pipeline_id}/updates\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {dbutils.secrets.get('myscope', 'databricks-token')}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    response = requests.post(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Pipeline started successfully\")\n",
    "    else:\n",
    "        print(f\"Failed to start pipeline: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "312f6ff2-50c1-41ed-a356-bbe16ef37b1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Sample Job JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77ce835f-c842-474c-bc23-d75db76e0b4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"name\": \"job_farmers_functional_test\",\n",
    "  \"email_notifications\": {\n",
    "    \"on_success\": [\n",
    "      \"chchukwu@microsoft.com\"\n",
    "    ],\n",
    "    \"on_failure\": [\n",
    "      \"chchukwu@microsoft.com\"\n",
    "    ],\n",
    "    \"no_alert_for_skipped_runs\": false\n",
    "  },\n",
    "  \"webhook_notifications\": {},\n",
    "  \"notification_settings\": {\n",
    "    \"no_alert_for_skipped_runs\": false,\n",
    "    \"no_alert_for_canceled_runs\": false\n",
    "  },\n",
    "  \"timeout_seconds\": 0,\n",
    "  \"max_concurrent_runs\": 1,\n",
    "  \"tasks\": [\n",
    "    {\n",
    "      \"task_key\": \"process_data\",\n",
    "      \"run_if\": \"ALL_SUCCESS\",\n",
    "      \"notebook_task\": {\n",
    "        \"notebook_path\": \"/Workspace/Users/admin@mngenvmcap374485.onmicrosoft.com/dlt-helpers/functional-testing/farmers/generate_data\",\n",
    "        \"source\": \"WORKSPACE\"\n",
    "      },\n",
    "      \"job_cluster_key\": \"Job_cluster\",\n",
    "      \"timeout_seconds\": 0,\n",
    "      \"email_notifications\": {},\n",
    "      \"webhook_notifications\": {}\n",
    "    },\n",
    "    {\n",
    "      \"task_key\": \"dlt_farmers\",\n",
    "      \"depends_on\": [\n",
    "        {\n",
    "          \"task_key\": \"process_data\"\n",
    "        }\n",
    "      ],\n",
    "      \"run_if\": \"ALL_SUCCESS\",\n",
    "      \"pipeline_task\": {\n",
    "        \"pipeline_id\": \"1e3765b1-79a4-47e7-82b9-885c037d2fde\",\n",
    "        \"full_refresh\": false\n",
    "      },\n",
    "      \"timeout_seconds\": 0,\n",
    "      \"email_notifications\": {},\n",
    "      \"webhook_notifications\": {}\n",
    "    },\n",
    "    {\n",
    "      \"task_key\": \"query_expectations\",\n",
    "      \"depends_on\": [\n",
    "        {\n",
    "          \"task_key\": \"dlt_farmers\"\n",
    "        }\n",
    "      ],\n",
    "      \"run_if\": \"ALL_SUCCESS\",\n",
    "      \"notebook_task\": {\n",
    "        \"notebook_path\": \"/Workspace/Users/admin@mngenvmcap374485.onmicrosoft.com/dlt-helpers/functional-testing/farmers/query_expectations_events\",\n",
    "        \"base_parameters\": {\n",
    "          \"test_param\": \"test_val\"\n",
    "        },\n",
    "        \"source\": \"WORKSPACE\"\n",
    "      },\n",
    "      \"job_cluster_key\": \"Job_cluster\",\n",
    "      \"timeout_seconds\": 0,\n",
    "      \"email_notifications\": {},\n",
    "      \"notification_settings\": {\n",
    "        \"no_alert_for_skipped_runs\": false,\n",
    "        \"no_alert_for_canceled_runs\": false,\n",
    "        \"alert_on_last_attempt\": false\n",
    "      },\n",
    "      \"webhook_notifications\": {}\n",
    "    }\n",
    "  ],\n",
    "  \"job_clusters\": [\n",
    "    {\n",
    "      \"job_cluster_key\": \"Job_cluster\",\n",
    "      \"new_cluster\": {\n",
    "        \"cluster_name\": \"\",\n",
    "        \"spark_version\": \"15.4.x-scala2.12\",\n",
    "        \"azure_attributes\": {\n",
    "          \"first_on_demand\": 1,\n",
    "          \"availability\": \"ON_DEMAND_AZURE\",\n",
    "          \"spot_bid_max_price\": -1\n",
    "        },\n",
    "        \"node_type_id\": \"Standard_D4ds_v5\",\n",
    "        \"spark_env_vars\": {\n",
    "          \"PYSPARK_PYTHON\": \"/databricks/python3/bin/python3\"\n",
    "        },\n",
    "        \"enable_elastic_disk\": true,\n",
    "        \"data_security_mode\": \"USER_ISOLATION\",\n",
    "        \"runtime_engine\": \"PHOTON\",\n",
    "        \"num_workers\": 8\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"queue\": {\n",
    "    \"enabled\": true\n",
    "  },\n",
    "  \"parameters\": [\n",
    "    {\n",
    "      \"name\": \"functional_test\",\n",
    "      \"default\": \"farmers\"\n",
    "    }\n",
    "  ],\n",
    "  \"run_as\": {\n",
    "    \"user_name\": \"admin@mngenvmcap374485.onmicrosoft.com\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90f3bc43-9c18-4df4-9320-1236d94d639b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DLT API: https://docs.databricks.com/api/azure/workspace/pipelines/create\n",
    "\n",
    "# Jobs API: https://docs.databricks.com/en/reference/jobs-2.0-api.html#run-now\n",
    "\n",
    "# Solution Idea: https://stackoverflow.com/questions/74523740/lambda-to-trigger-a-dlt-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d5122a9-4f2a-4da2-b0a8-f4f3ebcdf121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%yaml\n",
    "pipelines:\n",
    "  - name: \"farmers_pipeline\"\n",
    "    id: \"pipeline_id_1\"\n",
    "  - name: \"retail_pipeline\"\n",
    "    id: \"pipeline_id_2\"\n",
    "  - name: \"okta_pipeline\"\n",
    "    id: \"pipeline_id_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33a843a9-231e-4b79-bfd1-44c00c2d70fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Function to read the YAML file\n",
    "def read_yaml(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "    \n",
    "# Read the YAML file\n",
    "config = read_yaml(\"/dbfs/path/to/pipeline_config.yaml\")\n",
    "\n",
    "# Get the selected functional test\n",
    "selected_test = dbutils.widgets.get(\"functional_test\")\n",
    "\n",
    "# Find the pipeline ID for the selected test\n",
    "pipeline_id = None\n",
    "for pipeline in config['pipelines']:\n",
    "    if pipeline['name'].startswith(selected_test):\n",
    "        pipeline_id = pipeline['id']\n",
    "        break\n",
    "\n",
    "# Start the DLT pipeline if the ID is found\n",
    "if pipeline_id:\n",
    "    start_dlt_pipeline(pipeline_id)\n",
    "else:\n",
    "    print(f\"No pipeline found for the selected test: {selected_test}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "jobs_controller_loop",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
