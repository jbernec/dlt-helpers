{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ecd065f-b459-42ec-911b-4c361472dbd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "USE CATALOG dlt_catalog;\n",
    "USE SCHEMA dlt_schema;\n",
    "CREATE or Replace VIEW event_log_raw AS SELECT * FROM event_log(TABLE(dlt_catalog.dlt_schema.raw_farmers_market));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d60b0f0b-f171-4770-a998-8d17006fd7b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_events = spark.sql(\"\"\"\n",
    "          SELECT * FROM event_log(TABLE(dlt_catalog.dlt_schema.raw_farmers_market))\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91a4cead-2570-4e04-8913-077be80f71e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_events.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b83c15e-d5b3-4497-ba76-114686d88314",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--select * from event_log_raw;\n",
    "\n",
    "SELECT id, `timestamp`, origin.flow_name, details:flow_progress.metrics.num_output_rows, details:flow_progress.status, details:flow_progress.data_quality.dropped_records, details:flow_progress.data_quality.expectations, * FROM event_log_raw WHERE event_type = 'flow_progress' ORDER BY timestamp DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7de9d3ec-d45c-401d-935c-159f19ceb7e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_events.selectExpr(\n",
    "    \"id\",\n",
    "    \"timestamp\",\n",
    "    \"origin.flow_name\",\n",
    "    \"details:flow_progress.metrics.num_output_rows\",\n",
    "    \"details:flow_progress.status\",\n",
    "    \"details:flow_progress.data_quality.dropped_records\",\n",
    "    \"details:flow_progress.data_quality.expectations\",\n",
    "    \"*\",\n",
    ").filter(\"event_type = 'flow_progress' and details:flow_progress.status = 'COMPLETED'\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d4a6dca-f520-4cfd-826c-b0ca80a4b620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_expectations = df_events.selectExpr(\"id\", \"details:flow_progress.data_quality.expectations\").filter(\"event_type = 'flow_progress' and details:flow_progress.status = 'RUNNING'\")\n",
    "\n",
    "df_expectations.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afe2ff1d-2ccc-4308-94ed-4d75fe4f41a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    ArrayType,\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    ")\n",
    "from pyspark.sql.functions import explode, from_json\n",
    "\n",
    "schema = ArrayType(\n",
    "    StructType(\n",
    "        [\n",
    "            StructField(\"name\", StringType()),\n",
    "            StructField(\"dataset\", StringType()),\n",
    "            StructField(\"passed_records\", IntegerType()),\n",
    "            StructField(\"failed_records\", IntegerType()),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "df_expectations = df_expectations.withColumn(\"expectations\", explode(from_json(df_expectations.expectations, schema)))\n",
    "df_expectations = df_expectations.selectExpr(\"id\", \"expectations.name\",  \"expectations.dataset\", \"expectations.passed_records\", \"expectations.failed_records\")\n",
    "\n",
    "df_expectations.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85bb1a93-a001-47b8-a973-e8d6f7497878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fc8ec0a-5039-408c-8a92-4b4d7af24a53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW latest_update AS SELECT origin.update_id AS id FROM event_log_raw WHERE event_type = 'create_update' ORDER BY timestamp DESC LIMIT 1;\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc8977eb-7fce-461d-8443-ac82530fa148",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "          select * from latest_update;\n",
    "          \"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52b9bf28-546c-499c-831d-c6cacf256e3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  row_expectations.dataset as dataset,\n",
    "  row_expectations.name as expectation,\n",
    "  SUM(row_expectations.passed_records) as passing_records,\n",
    "  SUM(row_expectations.failed_records) as failing_records\n",
    "FROM\n",
    "  (\n",
    "    SELECT\n",
    "      explode(\n",
    "        from_json(\n",
    "          details :flow_progress.data_quality.expectations,\n",
    "          \"array<struct<name: string, dataset: string, passed_records: int, failed_records: int>>\"\n",
    "        )\n",
    "      ) row_expectations\n",
    "    FROM\n",
    "      event_log_raw,\n",
    "      latest_update\n",
    "    WHERE\n",
    "      event_type = 'flow_progress'\n",
    "      AND origin.update_id = latest_update.id\n",
    "  )\n",
    "GROUP BY\n",
    "  row_expectations.dataset,\n",
    "  row_expectations.name\n",
    "  \"\"\")\n",
    "\n",
    "# # Write the DataFrame to a Unity Catalog table\n",
    "# df.write.format(\"delta\").saveAsTable(\"dlt_catalog.dlt_schema.quality_metrics\")\n",
    "\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3140012656321377,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "query_expectations_events",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
