{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fefba495-d8ef-45dd-89e3-a8b6720fd6f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Permission is based on File or folder based ACL assignments to the Data Lake filesystem (container) . RBAC assignments to the top level Azure Data Lake resource is not required.\n",
    "# https://docs.databricks.com/storage/azure-storage.html\n",
    "spark.conf.set(\"fs.azure.account.auth.type.adls04.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.adls04.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.adls04.dfs.core.windows.net\", dbutils.secrets.get(\"myscope\", key=\"clientid\"))\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.adls04.dfs.core.windows.net\", dbutils.secrets.get(\"myscope\", key=\"clientsecret\"))\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.adls04.dfs.core.windows.net\", \"https://login.microsoftonline.com/{}/oauth2/token\".format(dbutils.secrets.get(\"myscope\", key=\"tenantid\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2343c583-ac0a-4b6b-a50c-771e61314409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the source, target paths and options\n",
    "\n",
    "schema_location = \"abfss://unity-catalog@adls04.dfs.core.windows.net/csvdata/_schematracking\"\n",
    "source_path = \"/tmp/delta/concat\"\n",
    "\n",
    "options = {\n",
    "    \"cloudFiles.format\": \"parquet\",\n",
    "    \"cloudFiles.inferColumnTypes\": False,\n",
    "    \"cloudFiles.inferSchema\": False,\n",
    "    \"cloudFiles.schemaEvolutionMode\":\"rescue\",\n",
    "    \"cloudFiles.schemaLocation\": schema_location,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa0457f1-5652-475b-8f84-3b45bb89f453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from helper_functions import get_rules_as_list_of_dict, remove_at_symbol, get_rules\n",
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "# UC enabled DLT bronze ingestion with schema inference\n",
    "@dlt.table(\n",
    "    table_properties={\"quality\": \"bronze\"},\n",
    "    comment=\"This table contains raw farmers market data with '@' removed from the website column\",\n",
    "    name=\"raw_farmers_market\")\n",
    "@dlt.expect_all(get_rules('character_validity'))\n",
    "def get_data():\n",
    "    df = (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .options(**options)\n",
    "        .option(\"cloudFiles.schemaLocation\", schema_location)  # Specify the schema location\n",
    "        .load(source_path)\n",
    "    )\n",
    "\n",
    "    # Apply the remove_at_symbol function to the 'website' column\n",
    "    df = remove_at_symbol(df, \"website\")\n",
    "    \n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dlt-integration-pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
